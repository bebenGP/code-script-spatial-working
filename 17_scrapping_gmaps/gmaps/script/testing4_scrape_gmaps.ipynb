{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kakot</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kota Administrasi Jakarta Selatan</td>\n",
       "      <td>0.848568</td>\n",
       "      <td>0.011820</td>\n",
       "      <td>-6.272567</td>\n",
       "      <td>106.810043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kota Bogor</td>\n",
       "      <td>0.708177</td>\n",
       "      <td>0.009106</td>\n",
       "      <td>-6.593494</td>\n",
       "      <td>106.799357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kota Padang</td>\n",
       "      <td>1.881366</td>\n",
       "      <td>0.056664</td>\n",
       "      <td>-0.899532</td>\n",
       "      <td>100.437912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Magelang</td>\n",
       "      <td>2.016873</td>\n",
       "      <td>0.093674</td>\n",
       "      <td>-7.502515</td>\n",
       "      <td>110.249411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pacitan</td>\n",
       "      <td>2.715767</td>\n",
       "      <td>0.117653</td>\n",
       "      <td>-8.124997</td>\n",
       "      <td>111.178150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               kakot  Shape_Leng  Shape_Area       lat  \\\n",
       "0  Kota Administrasi Jakarta Selatan    0.848568    0.011820 -6.272567   \n",
       "1                         Kota Bogor    0.708177    0.009106 -6.593494   \n",
       "2                        Kota Padang    1.881366    0.056664 -0.899532   \n",
       "3                           Magelang    2.016873    0.093674 -7.502515   \n",
       "4                            Pacitan    2.715767    0.117653 -8.124997   \n",
       "\n",
       "         long  \n",
       "0  106.810043  \n",
       "1  106.799357  \n",
       "2  100.437912  \n",
       "3  110.249411  \n",
       "4  111.178150  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = r\"D:\\SCRIPT - ARCPY - PYTHON\\script-python-working-in-esri\\17_scrapping_gmaps\\gmaps\\input\\kabkot_list_and_xy_sample.csv\"\n",
    "data = r\"C:\\Users\\bputra\\OneDrive - ESRI Indonesia\\PT ESRI Indonesia\\Research\\code-script\\17_scrapping_gmaps\\gmaps\\input\\kabkot_list_and_xy_sample.csv\"\n",
    "df = pd.read_csv(data, delimiter=\";\")\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lat = df['lat']\n",
    "data_long = df['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "\n",
    "# Inisialisasi WebDriver Chrome\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scroll_to_bottom(iteration=None):\n",
    "    xpath_sidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]')\n",
    "    if(iteration == None):\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        for i in range(1, iteration):\n",
    "            driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "            time.sleep(2)\n",
    "\n",
    "def scrape_data():\n",
    "    # Daftar XPath untuk setiap informasi\n",
    "    xpath_urls = [\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/a',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[1]/div[2]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[3]/div/span[2]/span/span[1]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[1]/span[1]/span',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[1]/span[2]/span[2]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[2]/span[2]/span[2]'\n",
    "    ]\n",
    "    xpath_href = ['/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/a']\n",
    "    #xpath_href = ['//a[@class=\"hfpxzc\"]']\n",
    "    \n",
    "    # List untuk menyimpan semua data\n",
    "    all_data = []\n",
    "\n",
    "    # Loop untuk mengekstrak informasi dari setiap elemen\n",
    "    for i in range(3, 30, 2):  # misalnya, untuk nomor 3, 5, 7, 9, 11\n",
    "        for xpath in xpath_urls:\n",
    "            xpath = xpath.format(i)\n",
    "            #print(xpath)\n",
    "            try :\n",
    "                element = driver.find_element(By.XPATH, xpath)\n",
    "                information = element.text\n",
    "                all_data.append((information))\n",
    "            except NoSuchElementException:\n",
    "                # Jika elemen tidak ditemukan, tambahkan string kosong ke dalam list\n",
    "                all_data.append('')\n",
    "                \n",
    "        for xpath_xy in xpath_href:\n",
    "            xpath_xy = xpath_xy.format(i)\n",
    "            try:\n",
    "                elements = driver.find_elements(By.XPATH, xpath_xy)\n",
    "                for element_xy in elements:\n",
    "                    information_href = element_xy.get_attribute('href')\n",
    "                    #information_data = element.get_attribute('aria-label')\n",
    "                    all_data.append((information_href))\n",
    "            except NoSuchElementException:\n",
    "                # Jika elemen tidak ditemukan, tambahkan string kosong ke dalam list\n",
    "                all_data.append(('',''))\n",
    "\n",
    "    # Cetak semua data yang telah diambil\n",
    "    for info in all_data:\n",
    "        print(info)\n",
    "\n",
    "def url_hit(data_latitude, data_longitude) :\n",
    "    try : \n",
    "        for latitude, longitude in zip (data_latitude, data_longitude) :\n",
    "            start_time = time.time()\n",
    "            try : \n",
    "                url = f'https://www.google.com/maps/search/Bulutangkis/@{latitude},{longitude},12.58z?entry=ttu' \n",
    "                print(f\"Start in {url}\")\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                try :\n",
    "                    for i in range(1, 100):\n",
    "                        # scroll down to bottom sidebar\n",
    "                        scroll_to_bottom()\n",
    "\n",
    "                        # xpath_endsidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')\n",
    "                        if(check_exists_by_xpath('/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')):\n",
    "                            scroll_to_bottom(2)\n",
    "                            print(\"End scroll down !!\")\n",
    "                            \n",
    "                            break\n",
    "                \n",
    "                except Exception as e :\n",
    "                    print(f\"Ada error ni di request {url} -> {e}\")\n",
    "                    \n",
    "                print(f'url {url} sudah selesai load')\n",
    "                \n",
    "                try :\n",
    "                    #start crawling data !\n",
    "                    scrape_data()\n",
    "                except Exception as e :\n",
    "                    print(\"Terdapat error dibagian Crawling data(Function scrape_data)\")\n",
    "                \n",
    "            except TimeoutError :\n",
    "                print(f'Teeett habis waktu jadi error di {url}')\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "            execution_time = (end_time - start_time)/60\n",
    "            print(f\"Waktu yang dibutuhkan {execution_time:.2f} menit\")\n",
    "\n",
    "        print(\"Data sudah selesai di scrapping, horee !!!\")\n",
    "        \n",
    "    except Exception as e :\n",
    "        print('error nih haduhhh')\n",
    "\n",
    "# URL target\n",
    "#driver.get(url)\n",
    "# Panggil fungsi scrape_data untuk mengeksekusi scraping\n",
    "#url_hit(data_lat,data_long)\n",
    "# Tutup WebDriver setelah selesai\n",
    "#driver.quit()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start in https://www.google.com/maps/search/Bulutangkis/@-6.2725674,106.810043,12.58z?entry=ttu\n",
      "End scroll down !!\n",
      "url https://www.google.com/maps/search/Bulutangkis/@-6.2725674,106.810043,12.58z?entry=ttu sudah selesai load\n",
      "Terdapat error dibagian Crawling data(Function scrape_data)\n",
      "Waktu yang dibutuhkan 0.90 menit\n",
      "Start in https://www.google.com/maps/search/Bulutangkis/@-6.59349435,106.7993574,12.58z?entry=ttu\n",
      "Ada error ni di request https://www.google.com/maps/search/Bulutangkis/@-6.59349435,106.7993574,12.58z?entry=ttu -> Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=121.0.6167.185)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF743127012+3522402]\n",
      "\t(No symbol) [0x00007FF742D48352]\n",
      "\t(No symbol) [0x00007FF742BF5ABB]\n",
      "\t(No symbol) [0x00007FF742BD287C]\n",
      "\t(No symbol) [0x00007FF742C65D97]\n",
      "\t(No symbol) [0x00007FF742C7B3CF]\n",
      "\t(No symbol) [0x00007FF742C5EE03]\n",
      "\t(No symbol) [0x00007FF742C2F4D4]\n",
      "\t(No symbol) [0x00007FF742C305F1]\n",
      "\tGetHandleVerifier [0x00007FF743159B9D+3730157]\n",
      "\tGetHandleVerifier [0x00007FF7431AF02D+4079485]\n",
      "\tGetHandleVerifier [0x00007FF7431A75D3+4048163]\n",
      "\tGetHandleVerifier [0x00007FF742E7A649+718233]\n",
      "\t(No symbol) [0x00007FF742D54A3F]\n",
      "\t(No symbol) [0x00007FF742D4FA94]\n",
      "\t(No symbol) [0x00007FF742D4FBC2]\n",
      "\t(No symbol) [0x00007FF742D3F2E4]\n",
      "\tBaseThreadInitThunk [0x00007FFBE8197344+20]\n",
      "\tRtlUserThreadStart [0x00007FFBEA0C26B1+33]\n",
      "\n",
      "url https://www.google.com/maps/search/Bulutangkis/@-6.59349435,106.7993574,12.58z?entry=ttu sudah selesai load\n",
      "Terdapat error dibagian Crawling data(Function scrape_data)\n",
      "Waktu yang dibutuhkan 0.54 menit\n",
      "Start in https://www.google.com/maps/search/Bulutangkis/@-0.89953247,100.4379122,12.58z?entry=ttu\n",
      "error nih haduhhh\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "import csv\n",
    "\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scroll_to_bottom(iteration=None):\n",
    "    xpath_sidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]')\n",
    "    if(iteration == None):\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        for i in range(1, iteration):\n",
    "            driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "            time.sleep(2)\n",
    "\n",
    "def save_to_csv(data_result_crawling, csv_file) :\n",
    "    #csv_file = 'scraped_data_testing5.csv'\n",
    "    \n",
    "    try :\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8') as file :\n",
    "            writer = csv.writer(file, delimiter=';')\n",
    "            writer.writerow(['poi_name', 'ratings', 'tags', 'address', 'telp', 'url'])\n",
    "            \n",
    "            for row in data_result_crawling:\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        print(f\"Data telah disimpan kedalam file csv : {csv_file}\")\n",
    "    except Exception as e :\n",
    "        print(f\"Terjadi Kesalahan saat menyimpan data kedalam CSV : {e}\")\n",
    "\n",
    "def scrape_data():\n",
    "    # Daftar XPath untuk setiap informasi\n",
    "    xpath_urls = [\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[1]/div[2]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[3]/div/span[2]/span/span[1]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[1]/span[1]/span',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[1]/span[2]/span[2]',\n",
    "        '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/div[2]/div[4]/div[1]/div/div/div[2]/div[4]/div[2]/span[2]/span[2]'\n",
    "    ]\n",
    "    xpath_href = ['/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[{}]/div/a']\n",
    "    #xpath_href = ['//a[@class=\"hfpxzc\"]']\n",
    "    \n",
    "    # List untuk menyimpan semua data\n",
    "    all_data = []\n",
    "\n",
    "    # Loop untuk mengekstrak informasi dari setiap elemen\n",
    "    for i in range(3, 30, 2):  # misalnya, untuk nomor 3, 5, 7, 9, 11\n",
    "        row_data = []\n",
    "        for xpath in xpath_urls:\n",
    "            xpath = xpath.format(i)\n",
    "            #print(xpath)\n",
    "            try :\n",
    "                element = driver.find_element(By.XPATH, xpath)\n",
    "                information = element.text\n",
    "                row_data.append((information))\n",
    "            except NoSuchElementException:\n",
    "                # Jika elemen tidak ditemukan, tambahkan string kosong ke dalam list\n",
    "                row_data.append('')\n",
    "                \n",
    "        for xpath_xy in xpath_href:\n",
    "            xpath_xy = xpath_xy.format(i)\n",
    "            try:\n",
    "                elements = driver.find_elements(By.XPATH, xpath_xy)\n",
    "                for element_xy in elements:\n",
    "                    information_href = element_xy.get_attribute('href')\n",
    "                    #information_data = element.get_attribute('aria-label')\n",
    "                    row_data_data.append(information_href)\n",
    "            except NoSuchElementException:\n",
    "                # Jika elemen tidak ditemukan, tambahkan string kosong ke dalam list\n",
    "                row_data.append('')\n",
    "\n",
    "        all_data.append(row_data, 'scraped_data_testing5.csv')\n",
    "        #return all_data\n",
    "        \n",
    "    save_to_csv(all_data)\n",
    "\n",
    "    # Cetak semua data yang telah diambil\n",
    "    for info in all_data:\n",
    "        print(info)\n",
    "\n",
    "def url_hit(data_latitude, data_longitude) :\n",
    "    try : \n",
    "        for latitude, longitude in zip (data_latitude, data_longitude) :\n",
    "            start_time = time.time()\n",
    "            try : \n",
    "                url = f'https://www.google.com/maps/search/Bulutangkis/@{latitude},{longitude},12.58z?entry=ttu' \n",
    "                print(f\"Start in {url}\")\n",
    "                driver.get(url)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                try :\n",
    "                    for i in range(1, 100):\n",
    "                        # scroll down to bottom sidebar\n",
    "                        scroll_to_bottom()\n",
    "\n",
    "                        # xpath_endsidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')\n",
    "                        if(check_exists_by_xpath('/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')):\n",
    "                            scroll_to_bottom(2)\n",
    "                            print(\"End scroll down !!\")\n",
    "                            \n",
    "                            break\n",
    "                \n",
    "                except Exception as e :\n",
    "                    print(f\"Ada error ni di request {url} -> {e}\")\n",
    "                    \n",
    "                print(f'url {url} sudah selesai load')\n",
    "                \n",
    "                try :\n",
    "                    #start crawling data !\n",
    "                    scrape_data()\n",
    "                except Exception as e :\n",
    "                    print(\"Terdapat error dibagian Crawling data(Function scrape_data)\")\n",
    "                \n",
    "            except TimeoutError :\n",
    "                print(f'Teeett habis waktu jadi error di {url}')\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "            execution_time = (end_time - start_time)/60\n",
    "            print(f\"Waktu yang dibutuhkan {execution_time:.2f} menit\")\n",
    "\n",
    "        print(\"Data sudah selesai di scrapping, horee !!!\")\n",
    "        \n",
    "    except Exception as e :\n",
    "        print('error nih haduhhh')\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "url_hit(data_lat, data_long)\n",
    "time.sleep(2)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
