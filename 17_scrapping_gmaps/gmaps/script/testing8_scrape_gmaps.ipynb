{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "import requests\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_exists_by_xpath(xpath):\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, xpath)\n",
    "    except:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def scroll_to_bottom(iteration=None):\n",
    "    xpath_sidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]')\n",
    "    if(iteration == None):\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "        time.sleep(2)\n",
    "    else:\n",
    "        for i in range(1, iteration):\n",
    "            driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", xpath_sidebar)\n",
    "            time.sleep(2)\n",
    "\n",
    "def url_hit(data_latitude, data_longitude) :\n",
    "    try : \n",
    "        data = []\n",
    "        for latitude, longitude in zip (data_latitude, data_longitude) :\n",
    "            start_time = time.time()\n",
    "            try : \n",
    "                url = f'https://www.google.com/maps/search/Bulutangkis/@{latitude},{longitude},12.58z?entry=ttu' \n",
    "                print(f\"Start in {url}\")\n",
    "                driver.get(url)\n",
    "                \n",
    "                time.sleep(2)\n",
    "                \n",
    "                try :\n",
    "                    for i in range(1, 100):\n",
    "                        # scroll down to bottom sidebar\n",
    "                        scroll_to_bottom()\n",
    "                        \n",
    "                        # xpath_endsidebar = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')\n",
    "                        if(check_exists_by_xpath('/html/body/div[2]/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[1]/div[1]/div[243]/div/p/span/span')):\n",
    "                            scroll_to_bottom(2)\n",
    "                            print(\"End scroll down !!\")\n",
    "                            try :\n",
    "                                # Dapatkan HTML halaman web setelah di-render\n",
    "                                html = driver.page_source\n",
    "                                # Gunakan BeautifulSoup untuk mengekstrak data\n",
    "                                soup = BeautifulSoup(html, 'lxml')\n",
    "                                tes = soup.find_all('div', class_='bJzME tTVLSc')\n",
    "                                #start crawling data !\n",
    "                                for cari in tes:\n",
    "                                    hrefs = cari.find_all('a', class_='hfpxzc')\n",
    "                                    for href in hrefs:\n",
    "                                        href_value = href.get('href')\n",
    "                                        data.append({'URL': href_value} if href_value else {'URL': '-'})\n",
    "                                        \n",
    "                                    poi_names = cari.find_all('div', class_='qBF1Pd fontHeadlineSmall')\n",
    "                                    for poi in poi_names:\n",
    "                                        poinames_value = poi.text.strip() \n",
    "                                        data.append({'Name': poinames_value} if poinames_value else {'Name': '-'})\n",
    "\n",
    "                                    ratings = cari.find_all('span', class_='MW4etd')\n",
    "                                    for rating in ratings:\n",
    "                                        ratings_value = rating.text.strip()\n",
    "                                        data.append({'Rating': ratings_value} if ratings_value else {'Rating': '-'})\n",
    "                                    \n",
    "                                    address_schema = cari.find_all('div', class_='Nv2PK tH5CWc THOPZb')\n",
    "                                    found_addresses = set()\n",
    "                                    for address_testing in address_schema :                \n",
    "                                        tagsaddress_elements = cari.find_all('div', class_='W4Efsd')\n",
    "                                        for tagsaddress1 in tagsaddress_elements :\n",
    "                                            tagsaddress1_value = tagsaddress1.find_all('div', class_='W4Efsd')\n",
    "                                            for tagsaddress2 in tagsaddress1_value :\n",
    "                                                tagsaddress2_value = tagsaddress2.findParents('div', class_='W4Efsd')\n",
    "                                                for tagsaddress3 in tagsaddress2_value :\n",
    "                                                    tagsaddress3_value = tagsaddress3.text.strip()\n",
    "                                                    # Kondisi untuk memeriksa apakah alamat sudah ditemukan sebelumnya\n",
    "                                                    if tagsaddress3_value not in found_addresses:\n",
    "                                                        found_addresses.add(tagsaddress3_value)\n",
    "                                                        data.append({'Address': tagsaddress3_value} if tagsaddress3_value else {'Address': '-'})\n",
    "\n",
    "                            except Exception as e :\n",
    "                                print(\"Terdapat error dibagian Crawling data(Function scrape_data)\")\n",
    "                            break\n",
    "                \n",
    "                except Exception as e :\n",
    "                    print(f\"Ada error ni di request {url} -> {e}\")\n",
    "                    \n",
    "                print(f'url {url} sudah selesai load')              \n",
    "    \n",
    "            except TimeoutError :\n",
    "                print(f'Teeett habis waktu jadi error di {url}')\n",
    "                \n",
    "            df = pd.DataFrame(data, columns=['URL','Name', 'Rating', 'Address'])\n",
    "            df.head()\n",
    "            print(f\"URL : {url} Finish\")\n",
    "            df.to_csv(r'D:\\SCRIPT - ARCPY - PYTHON\\script-python-working-in-esri\\17_scrapping_gmaps\\gmaps\\output\\testing1_scrape_gmaps_bulutangkis.csv',index=False, encoding='utf-8-sig',sep=';')\n",
    "                \n",
    "            end_time = time.time()\n",
    "            \n",
    "            execution_time = (end_time - start_time)/60\n",
    "            print(f\"Waktu yang dibutuhkan {execution_time:.2f} menit\")\n",
    "        \n",
    "        print(\"Data sudah selesai di scrapping, horee !!!\")\n",
    "    \n",
    "    except Exception as e :\n",
    "        print('error nih haduhhh')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = r\"D:\\SCRIPT - ARCPY - PYTHON\\script-python-working-in-esri\\17_scrapping_gmaps\\gmaps\\input\\kabkot_list_and_xy_sample.csv\"\n",
    "data = r\"D:\\SCRIPT - ARCPY - PYTHON\\script-python-working-in-esri\\17_scrapping_gmaps\\gmaps\\input\\kabkot_list_and_xy_bulutangkis.csv\"\n",
    "df = pd.read_csv(data, delimiter=\";\")\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lat = df['lat']\n",
    "data_long = df['long']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "url_hit(data_lat, data_long)\n",
    "time.sleep(2)\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
